% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/melsi_robust.R
\name{learn_melsi_metric_robust}
\alias{learn_melsi_metric_robust}
\title{Learn MeLSI Metric with Robust Ensemble}
\usage{
learn_melsi_metric_robust(
  X,
  y,
  B = 20,
  m_frac = 0.7,
  pre_filter = TRUE,
  filter_threshold = 0.1
)
}
\arguments{
\item{X}{A matrix of feature abundances with samples as rows and features as columns}

\item{y}{A vector of group labels for each sample}

\item{B}{Number of weak learners in the ensemble (default: 20)}

\item{m_frac}{Fraction of features to use in each weak learner (default: 0.7)}

\item{pre_filter}{Whether to apply pre-filtering to remove low-variance features (default: TRUE)}

\item{filter_threshold}{Threshold for pre-filtering feature selection (default: 0.1)}
}
\value{
A positive definite matrix representing the learned Mahalanobis distance metric.
The diagonal elements represent feature importance weights.
}
\description{
Learns an optimal Mahalanobis distance metric using ensemble learning for robust
microbiome data analysis. This is the core function that implements MeLSI's
metric learning algorithm.
}
\details{
The function implements an ensemble of weak learners that each learn a metric
on a bootstrap sample and feature subset. Key features:

\itemize{
\item \strong{Pre-filtering}: Removes low-variance features using t-tests
\item \strong{Bootstrap sampling}: Each learner uses a different sample subset
\item \strong{Feature subsampling}: Each learner uses a random subset of features
\item \strong{Ensemble combination}: Weak learners are weighted by performance
\item \strong{Robust optimization}: Gradient descent with early stopping
}

The learned metric matrix can be used to compute Mahalanobis distances that
are optimized for detecting group differences in the specific dataset.
}
\examples{
# Generate test data
test_data <- generate_test_data(n_samples = 60, n_taxa = 100, n_signal_taxa = 10)
X <- test_data$counts
y <- test_data$metadata$Group

# CLR transformation
X_clr <- X
X_clr[X_clr == 0] <- 1e-10
X_clr <- log(X_clr)
X_clr <- X_clr - rowMeans(X_clr)

# Learn metric
M_learned <- learn_melsi_metric_robust(X_clr, y, B = 50, m_frac = 0.7)

# Extract feature weights
feature_weights <- diag(M_learned)
cat("Top 5 feature weights:", sort(feature_weights, decreasing = TRUE)[1:5], "\n")
}
\seealso{
\code{\link{run_melsi_permutation_test}}, \code{\link{calculate_mahalanobis_dist_robust}}
}
